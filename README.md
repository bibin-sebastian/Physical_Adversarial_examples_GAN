

This Project will discuss the transferability of state of the art defense techniques for adversarial examples for deep learning systems in the physical domain. 
The project explores adversarial attacks using the Fast Gradient Sign Method (FGSM), Carlini \& Wagner (CW) and DeepFool attacks to 
generate adversarial images that are given to the classifier as a digital and physically transformed image. 
Furthermore,novel results are presented demonstrating the effectiveness of the state-of-the-art Defense-GAN technique to create reconstructions of images, that have undergone the physical transformation, with a significant portion of the adversarial noise filtered out. 
For finer adversarial attacks, the physical transformation itself causes a high degree of adversarial destruction, bringing to question the need for additional defenses. 




[Presentation](https://docs.google.com/presentation/d/12EdbAu67JD_41Jx1GSk2cmZ__yXMpJsvRCzBytWe93Q/edit#slide=id.g46f8c59f95_0_5)


**Acknowledgement**<br>
This repo is almost entirely made using the code by researchers -  Pouya Samangouei, Maya Kabkab, Rama Chellappa on their work Defense-GAN
https://arxiv.org/abs/1805.06605 <br>
https://github.com/kabkabm/defensegan

